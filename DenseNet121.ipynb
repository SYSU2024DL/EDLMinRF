{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db5cd4-9248-41f9-bb9d-acea5c90a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils import data \n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cf719-150e-4ec7-9701-d5d316588c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5,0.5,0.5],\n",
    "                                    std=[0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "test_transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5,0.5,0.5],\n",
    "                                    std=[0.5,0.5,0.5]),\n",
    "])\n",
    "\n",
    "verif_transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5,0.5,0.5],\n",
    "                                    std=[0.5,0.5,0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74cd04-d5f2-425e-8836-24b9995cb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=torchvision.datasets.ImageFolder(\n",
    "  'D:/train',\n",
    "    transform=train_transformer\n",
    ")\n",
    "\n",
    "verif_dataset=torchvision.datasets.ImageFolder(\n",
    "  'D:/verif',\n",
    "   transform=verif_transformer\n",
    ") \n",
    "\n",
    "test_dataset=torchvision.datasets.ImageFolder(\n",
    "  'D:/test',\n",
    "   transform=test_transformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d067f-5863-48db-b1d4-46ce231e1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8434e02-237a-488b-8dc0-a7ad4f676b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38708878-04cb-4283-a82f-ccea4fe06322",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_class={}\n",
    "for k,v in train_dataset.class_to_idx.items():\n",
    "    print(k,v)\n",
    "    id_to_class[v]=k\n",
    "id_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba65a8a6-be2e-4485-9144-3cb439091ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size=32\n",
    "dl_train=torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Batch_size,\n",
    "        shuffle=True\n",
    ")\n",
    "dl_verif=torch.utils.data.DataLoader(\n",
    "        verif_dataset,\n",
    "        batch_size=Batch_size,\n",
    "        shuffle=True\n",
    ")\n",
    "dl_test=torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=Batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd63d40-0fab-4caf-9a4d-acba0d2c2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet121=models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = densenet121.classifier.in_features\n",
    "densenet121.classifier = torch.nn.Linear(num_ftrs, 2)\n",
    "model=densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5abe2-00a3-44be-ad5f-01924a384008",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9c923-d59a-411c-81ca-99a04e9bf811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "optim=torch.optim.Adam(model.parameters(),lr=0.000005)\n",
    "scheduler = lr_scheduler.StepLR(optim,step_size=5,gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b805ff2-2b0b-43b3-85f9-7b04a010ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fd0e1-f324-4bf9-9ccc-9a7f048d5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epoch, model, trainloader, verifloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for x, y in tqdm(trainloader):\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.to('cuda'), y.to('cuda')\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            correct += (y_pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(trainloader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "        \n",
    "    verif_correct = 0\n",
    "    verif_total = 0\n",
    "    verif_running_loss = 0 \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(verifloader):\n",
    "            if torch.cuda.is_available():\n",
    "                x, y = x.to('cuda'), y.to('cuda')\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            verif_correct += (y_pred == y).sum().item()\n",
    "            verif_total += y.size(0)\n",
    "            verif_running_loss += loss.item()\n",
    "    \n",
    "    epoch_verif_loss = verif_running_loss / len(verifloader.dataset)\n",
    "    epoch_verif_acc = verif_correct / verif_total\n",
    "       \n",
    "    print('epoch: ', epoch, \n",
    "          'loss： ', round(epoch_loss, 3),\n",
    "          'accuracy:', round(epoch_acc, 3),\n",
    "          'verif_loss： ', round(epoch_verif_loss, 3),\n",
    "          'verif_accuracy:', round(epoch_verif_acc, 3))\n",
    "        \n",
    "    return epoch_loss, epoch_acc, epoch_verif_loss, epoch_verif_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0971a9-11ee-4871-83ff-8130492d2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71f316-0171-45e1-8b8b-4aa7875952d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 3  \n",
    "best_verif_loss = float('inf')\n",
    "best_verif_acc = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "verif_loss = []\n",
    "verif_acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, epoch_acc, epoch_verif_loss, epoch_verif_acc = fit(epoch,\n",
    "                                                                   model,\n",
    "                                                                   dl_train,\n",
    "                                                                   dl_verif)\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)\n",
    "    verif_loss.append(epoch_verif_loss)\n",
    "    verif_acc.append(epoch_verif_acc)\n",
    "\n",
    "    if epoch_verif_loss < best_verif_loss or epoch_verif_acc > best_verif_acc:\n",
    "        best_verif_loss = epoch_verif_loss\n",
    "        best_verif_acc  = epoch_verif_acc\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295fd00-4b11-4e9d-a26f-6ad9aa3caceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_loss)+1), train_loss, label='Train set', color='#0000FF')\n",
    "plt.plot(range(1, len(verif_loss)+1), verif_loss, label='Validation set', color='#FF0000')\n",
    "plt.title('Model loss function convergence curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "SFig1 = plt.gcf() \n",
    "SFig1.savefig(r'D:\\densenet_LOSS.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d532751-7553-4744-a632-4ea4b698b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs+1), train_acc, label='Train set', color='#0000FF')\n",
    "plt.plot(range(1, epochs+1), verif_acc, label='Validation set', color='#FF0000')\n",
    "plt.title('The accuracy of different deep learning nets in the training and validation set changes with epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accucary')\n",
    "plt.legend()\n",
    "Fig2 = plt.gcf() \n",
    "Fig2.savefig(r'D:\\densenet_acc.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614d303-a461-4272-a94d-740a596833db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dl_train:\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f976111-5435-4b35-a8dc-35bcc014a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds2 = []\n",
    "all_labels2 = []\n",
    "all_pred_probs2 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dl_verif:\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        outputs = model(inputs) \n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds2.extend(preds.cpu().numpy())\n",
    "        all_labels2.extend(labels.cpu().numpy())\n",
    "        all_pred_probs2.extend(probs.cpu().numpy())\n",
    "\n",
    "cm2 = confusion_matrix(all_labels2, all_preds2)\n",
    "print(cm2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c05bb3-9835-4394-be49-996aa970348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds1 = []\n",
    "all_labels1 = []\n",
    "all_pred_probs1 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dl_test:\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds1.extend(preds.cpu().numpy())\n",
    "        all_labels1.extend(labels.cpu().numpy())\n",
    "        all_pred_probs1.extend(probs.cpu().numpy())\n",
    "\n",
    "cm1 = confusion_matrix(all_labels1, all_preds1)\n",
    "print(cm1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5024ea62-d204-40f2-85df-0bd0ae15a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'densenet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649a8ef-4d2f-4c44-a49e-18b365e53f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8dc17-3c1b-4e64-9e26-1a0902feea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "TN =     # nunmber of True Negatives \n",
    "TP =     # nunmber of True Positives\n",
    "FP =     # nunmber of False Positives\n",
    "FN =     # nunmber of False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030cce9-b782-4610-869c-e41d53b823ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "NPV = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "def calc_confidence_interval(successes, trials, confidence=0.95):\n",
    "    ci = sm.stats.proportion_confint(successes, trials, alpha=1-confidence, method='wilson')\n",
    "    return ci\n",
    "\n",
    "accuracy_ci = calc_confidence_interval(TP + TN, TP + TN + FP + FN)\n",
    "sensitivity_ci = calc_confidence_interval(TP, TP + FN)\n",
    "specificity_ci = calc_confidence_interval(TN, TN + FP)\n",
    "NPV_ci = calc_confidence_interval(TN, TN + FN)\n",
    "precision_ci = calc_confidence_interval(TP, TP + FP)\n",
    "f1_score_ci = bootstrap_f1_score(TP, TN, FP, FN)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy_ci[0]:.4f}-{accuracy_ci[1]:.4f})\")\n",
    "print(f\"Sensitivity: {sensitivity:.4f} ({sensitivity_ci[0]:.4f}-{sensitivity_ci[1]:.4f})\")\n",
    "print(f\"Specificity: {specificity:.4f} ({specificity_ci[0]:.4f}-{specificity_ci[1]:.4f})\")\n",
    "print(f\"NPV: {NPV:.4f} ({NPV_ci[0]:.4f}-{NPV_ci[1]:.4f})\")\n",
    "print(f\"Precision: {precision:.4f} ({precision_ci[0]:.4f}-{precision_ci[1]:.4f})\")\n",
    "print(f\"F1-score: {f1_score:.4f} ({f1_score_ci[0]:.4f}-{f1_score_ci[1]:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
